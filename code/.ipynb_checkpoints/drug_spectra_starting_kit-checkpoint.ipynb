{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:843: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('D:\\\\3A\\\\Projet3A\\\\project\\\\abs_meet_summ\\\\code\\\\takahe')\n",
    "# os.chdir('C:\\\\Users\\\\mvazirg\\\\Documents\\\\abs_meet_summ\\\\code\\\\takahe')\n",
    "import time\n",
    "import takahe_params_tuning as compression\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "# we use TweetTokenizer because unlike word_tokenize it does not split contractions (e.g., didn't-> did n't)\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from lan_model import language_model\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading language model...\n",
      "finish loading language model, time_cost = 74.19s\n"
     ]
    }
   ],
   "source": [
    "# Create a language model\n",
    "print \"loading language model...\"\n",
    "start = time.time()\n",
    "# my_lm = language_model(model_path='C:\\\\Users\\\\mvazirg\\\\Documents\\\\en-70k-0.2.lm')\n",
    "my_lm = language_model(model_path='d:\\\\3A\\\\Projet3A\\\\project\\\\data\\\\en-70k-0.2.lm')\n",
    "elapse = time.time() - start\n",
    "print \"finish loading language model, time_cost = %.2fs\" % elapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "punct = string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### examples of clusters of well-formed sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### examples of clusters of well-formed sentences ####\n",
    "\n",
    "sentences=[\"Lonesome George, the world's last Pinta Island giant tortoise, has passed away\",\"The giant tortoise known as Lonesome George died Sunday at the Galapagos National Park in Ecuador.\", \"He was only about a hundred years old, but the last known giant Pinta tortoise, Lonesome George, has passed away.\", \"Lonesome George, a giant tortoise believed to be the last of his kind, has died.\"]\n",
    "\n",
    "# sentences = ['The wife of a former U.S. president Bill Clinton Hillary Clinton visited China last Monday','Hillary Clinton wanted to visit China last month but postponed her plans till Monday last week.','Hillary Clinton paid a visit to the People Republic of China on Monday.', 'Last week the Secretary of State Ms. Clinton visited Chinese officials.']\n",
    "\n",
    "# sentences = ['the meeting is about the design of a remote control','today, we will focus on the remote control','the production cost and price of the remote are two important parameters', 'design decisions will impact the price of the remote control',\"today's meeting deals with designing the remote control\",'the topic today is the remote control']\n",
    "\n",
    "# sentences = ['my favourite color is blue', 'do you like red?','I think red is a nice warm color', 'we need to decide about the colors',\"choosing the colors won't be easy, but blue is quite nice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### examples of clusters of utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### examples of clusters of utterances ####\n",
    "\n",
    "# path_to_comms = 'C:\\\\Users\\\\mvazirg\\\\Documents\\\\abs_meet_summ\\\\data\\\\datasets\\\\meeting_summarization\\\\ami_icsi\\\\communities\\\\ami\\\\'\n",
    "path_to_comms = 'D:\\\\3A\\\\Projet3A\\\\project\\\\abs_meet_summ\\\\data\\\\datasets\\\\meeting_summarization\\\\ami_icsi\\\\communities\\\\ami\\\\'\n",
    "\n",
    "with open(path_to_comms + 'IS1003a_comms.txt','r') as file:\n",
    "    sentences_all_comms = file.read().splitlines()\n",
    "\t\n",
    "# iterate through the sentences and do the splitting\n",
    "\n",
    "# list of lists\n",
    "comms = []\n",
    "comm = []\n",
    "\n",
    "for sentence in sentences_all_comms:\n",
    "    if sentence != '':\n",
    "        comm.append(sentence)\n",
    "    else:\n",
    "        comms.append(comm)\n",
    "        comm = []\n",
    "\n",
    "# retain communities with at least two sentences in them (for the others, compression is obviously not necessary)\n",
    "big_comms = [comm for comm in comms if len(comm)>1]\n",
    "\n",
    "sentences = big_comms[2]\n",
    "\n",
    "#### put sentences in the right format ####\n",
    "\n",
    "tagged_sentences = []\n",
    "for sentence in sentences:\n",
    "    tagged_sentence = []\n",
    "    tokens = my_tokenizer.tokenize(sentence)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    for tuple in tagged_tokens:\n",
    "        if tuple[1] in punct:\n",
    "            tagged_sentence.append('/'.join([tuple[0],'PUNCT']))\n",
    "        else:\n",
    "            tagged_sentence.append('/'.join(tuple))\n",
    "    tagged_sentences.append(' '.join(tagged_sentence))\n",
    "\n",
    "\n",
    "lists_of_tokens = [sent.split(' ') for sent in sentences]\n",
    "\n",
    "lists_of_tokens_flatten = [item for sublist in lists_of_tokens for item in sublist]\n",
    "\n",
    "lotf=lists_of_tokens_flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### intersect with GoogleNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading GoogleNews...\n",
      "finish loading GoogleNews, time_cost = 105.46s\n"
     ]
    }
   ],
   "source": [
    "vectors = Word2Vec(size=3e2, min_count=1)\n",
    "vectors.build_vocab(lists_of_tokens)\n",
    "\n",
    "path_to_wv = 'D:\\\\3A\\\\Projet3A\\\\project\\\\data\\\\' # to fill\n",
    "\n",
    "print \"loading GoogleNews...\"\n",
    "start = time.time()\n",
    "vectors.intersect_word2vec_format(path_to_wv + 'GoogleNews-vectors-negative300.bin.gz', binary=True) \n",
    "elapse = time.time() - start\n",
    "print \"finish loading GoogleNews, time_cost = %.2fs\" % elapse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Create a word graph from the set of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_type=1.0, word_attraction=0.0, keyphrase=0.0, fl_score=0.0, core_rank=0.0, word_embed=0.0, pos_filtering=0.0 \n",
      "start building graph...\n",
      "build graph with word_net\n",
      "finish buiding graph !\n",
      "13.6523809524 so the remote control to be international and not too\n",
      "14.9857142857 as you will discuss about the remote control so the\n",
      "15.6523809524 you really the remote control to be international and not too\n",
      "16.9857142857 as you will discuss about the remote control as we want\n",
      "16.9857142857 so the remote control has to be original trendy and user-friendly\n",
      "18.9523809524 it be an international product so you don't have to\n",
      "18.9857142857 you really the remote control has to be original trendy and user-friendly\n",
      "19.6523809524 so the remote control to be international product so you don't have to\n",
      "20.8571428571 and when designing u . v . remote control as we want\n",
      "21.619047619 it be an international remote control has to be original trendy and user-friendly\n",
      "graph_type=1.0, word_attraction=0.0, keyphrase=0.0, fl_score=1.0, core_rank=1.0, word_embed=1.0, pos_filtering=0.0 \n",
      "start building graph...\n",
      "build graph with word_net\n",
      "finish buiding graph !\n",
      "diversity score: [ 2.  2.  2.  2.  2.  1.  2.  2.  2.  2.  3.  2.  3.  2.  2.  2.  3.  2.\n",
      "  3.  2.  2.  3.  2.  2.  2.  3.  3.  2.  2.  3.  2.  2.  3.  2.  2.  3.\n",
      "  3.  3.  2.  2.  3.  2.  3.]\n",
      "13.6523809524 so the remote control to be international and not too\n",
      "14.9857142857 as you will discuss about the remote control so the\n",
      "15.6523809524 you really the remote control to be international and not too\n",
      "16.9857142857 as you will discuss about the remote control as we want\n",
      "16.9857142857 so the remote control has to be original trendy and user-friendly\n",
      "18.9523809524 it be an international product so you don't have to\n",
      "18.9857142857 you really the remote control has to be original trendy and user-friendly\n",
      "19.6523809524 so the remote control to be international product so you don't have to\n",
      "20.8571428571 and when designing u . v . remote control as we want\n",
      "21.619047619 it be an international remote control has to be original trendy and user-friendly\n"
     ]
    }
   ],
   "source": [
    "graph_type = [1,1]\n",
    "word_attraction = [0,1]\n",
    "keyphrase = [0,0]\n",
    "fl_score = [0,1]\n",
    "core_rank = [0,1]\n",
    "word_embed = [0,1]\n",
    "pos_filtering = [0,0]\n",
    "\n",
    "for i in range(len(graph_type)):\n",
    "    print \"graph_type=%.1f, word_attraction=%.1f, keyphrase=%.1f, fl_score=%.1f, core_rank=%.1f, word_embed=%.1f, pos_filtering=%.1f \" % (graph_type[i], word_attraction[i], keyphrase[i], fl_score[i], core_rank[i], word_embed[i], pos_filtering[i])\n",
    "\n",
    "    compresser = compression.word_graph(tagged_sentences,model=my_lm, vectors=vectors, lotf=lotf,graph_type=graph_type[i], word_attraction=word_attraction[i],keyphrase=keyphrase[i],fl_score=fl_score[i],core_rank=core_rank[i],word_embed=word_embed[i],num_cluster=5, domain=True, nb_words=10,lang='en',punct_tag=\"PUNCT\", pos_separator='/', cr_w = 10, cr_weighted = True, cr_pos_filtering = pos_filtering[i], cr_stemming = False)\n",
    "\n",
    "    # Write the word graph in the dot format\n",
    "    # compresser.write_dot('new.dot')\n",
    "\n",
    "    # Get the 50 best paths\n",
    "    candidates = compresser.get_compression(200)\n",
    "\n",
    "    final_paths = compresser.final_score(candidates,10)\n",
    "\n",
    "    for i in range(len(final_paths))[:10]:\n",
    "        print final_paths[i][0], final_paths[i][1]\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
